# Functions in Probability

## Type of Random Variable

We have 2 types of random variable:

- **Discrete**
  
  Discrete random variable have outcomes at discrete intervals. As in the case of rolling a dice, where the outcomes were countable and discrete (1,2,3 ... 6). Here we can usually specify the probability of each outcome in a tabular maner.

- **Continuous**
  
  Continuous random variable have outcomes belonging to a given specific range. They are usually rational and uncountable and span the whole range. Like in the case of Rainfall, which have any outcome in the range 0 to 100mm.

## Probability Distribution, PMF and PDF

It is an alternative way of representing a random variable and the probability of its outcomes. It is a function which when given an outcome of a random variable gives the probability of that random variable.

For a discrete random variable we can show the probability distribution in a tabular form or list the outcomes and the probability for each outcome. This function which describes the probability distribution for a discrete random variable is called the **Probability Mass Function** of that random variable.

For a continuous random variable this function is termed as the **Probability Density** of that random variable. 

**Note:** The output value by the PDF for a given value of continuous random variable is actually the *relative likelihood* the probability would achieve and not the absolute value. This is because in case of a continuous random variable, the values it can take are infinite in a given range, as a result the probability for a single value is not defined or can be said as 0. 

For a small interval $(x, x+ dx)$ and PDF being $p(x)$ the probability is given as:

$ p(x)dx$ where $dx \rightarrow 0$ 

We can only find the probability for a given specific range which is a subset of the actual range the random variable can take. The probability for a given sub range is calculated using the integral of the PDF in that range.

$$
p(x \in (a,b)) = \int_a^b p(x)dx \tag{1}
$$

## Conditions of PDF

As the Probability Density function is giving the probability for a random variable falling in a given range, it has 2 main properties extended from the normal proabability concepts:

- $$
  p(x) \ge 0; \forall x \in (-\infin, +\infin) 
\tag{2}
  $$
  
  This denotes that the probability for any value of $x$ will never be -ve.

- $$
  \int_{-\infin}^{+\infin}p(x)dx = 1 \tag{3}
  $$
  
  This denotes that the total probability of any random variable can be found by integrating the PDF from $-\infin$ to $+\infin$ and the total probability in any case will be equal to 1.

## Cumulative Distribution Function

The probability of $x$ in the range $(-\infin, z)$ is given by:

$$
P(z) = \int_{-\infin}^z p(x)dx \tag{4}
$$

This function is also called the Cumulative Distribution Function 

## Bayes Theorem

Bayes theorem can also be applied to continuous random variable:

$$
\begin{aligned}
p(x) & = \int p(x,y)dy \tag{5}\\
p(x,y) & = p(y|x)p(x)
\end{aligned}
$$

## Expectation

This is defined as the value that we can expect from a given random variable for a given experiment. It is the weighted average of the outcomes of the random variable, weighed using the probability found in the experiment. It is also called the **Observed value**  or the **Mean** of the random variable. It is denoted using the symbol **$\mu$**.

**Intuition:** We can think of Expectation as the value that we can believe we can get in the next event or what value can be changed with all the outcomes so far so as to get the same distribution that we have observed so far i.e. the outcomes having more probability has more chances to come and the outcomes having least probabilities have the least chance to come in the next event. Thus when calculating the Expectation, the more probable outcomes add more value to Expectation and least probable outcomes add the least.

For a given experiment conducted $N$ times the Expectation for a discrete random variable can be defines as:

$$
\tag{6}
\mu = \mathbb{E}(x) = \sum_i x_i \times p(x_i)
$$

for a continuous random variable it is:

$$
\mathbb{E}(x) = \int x \times p(x)dx \tag{7}
$$

In case of conditional probabilities:

$$
\mathbb{E}(y|x) = \sum_i y_i\times p(y_i|x)
$$

For a function $f(x)$ where $x$ is the random variable we can define the Expected value as:

$$
E(f(x)) = \sum_i f(x_i)p(x_i) \tag{8}
$$

We will soon se why we need to define $eq. 8$.



## Standard Deviation, Variation and Covariance

Standard deviation is the mesure of the amount of dipsersion of the outcomes with reference to the Expected value. The lesser the standard deviation the more likely we can expect the outcome to be the Expected value.

> The standard deviation with the Expected value gives us a basic understanding of the experiment and its outcomes.

It is denoted using the symbol $\sigma$ and calculated as the average of the absolute difference of each outcome with the Expected value.

$$
\sigma(x) = \frac{1}{N} \sum_i |x_i - E[X]| \tag{9}
$$

The variance of an experiment is the Expected value of the squared deviation from the Mean i.e.

$$
\begin{aligned}
Var(x) & = E([x - \mu]^2) \\
& = \sum_i (x_i - \mu)^2p(x_i) & [\text{using eq.8}]\\
& = \sum_i (x_i^2 + \mu^2 - 2\mu x_i)p(x_i) \\ 
& = \sum_i (x_i^2p(x_i) + \mu^2p(x_i) - 2\mu x_ip(x_i)) \\ 
& = E(x^2) + \mu^2 - 2\mu\times\mu \\
& = E(x^2) - \mu^2 \\
& = E(x^2) - [E(x)]^2  \tag{10}
\end{aligned}
$$

The function variance when defined for 2 or more random variable is known as the **CoVariance** of those 2 random variable:

$$
cov(x,y) = \mathbb{E}(x,y) - \mathbb{E}(x)\mathbb{E}(y) \tag{11}
$$





# Reference

[1]: Wikipedia

[2]: Book "Pattern Recogniton and MachineLearning by Christopher M Bishop"


