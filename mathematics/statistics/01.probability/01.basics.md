# Introduction to Probability

For understanding purposes we will be using 2 dice of differenct shapes as our examples.

And also generalize the conccepts using a generalized example.

<img src='../../assets/dice.jpg' />

## Basic Rules and Notations

Lets say we take a normal dice of 6 sides with the number 1 to 6 on it and a dodecahedron (3d shape with 12 sides and has a pentagonal face) with the number 1 to 12 on it.

And thus we can also consider a generalized scenario where we have multiple dices $R_1,R_2,\cdots R_n$ each having different shapes and different no of faces.

### Mutually Exclusive

Here the rolling of 1 dice doesnt affect the outcome of the rolling of another dice. As a result we roll both the dice in any order and in any way, i.e. rolling 1 dice before the another or at the same time will always result in the same outcome. Thus we can say that the activity of rolling the dices are **mutually exclusive**.

### Random Variable, Events and Experiment

When we roll both the dices, we get 2 outcomes:

- 1st dice with the number in the range 1 to 6
- 2nd dice with the number in the range 1 to 12

These dices when will give different numbers as and when we roll them. Let say we assign the variable $X$ to the oucome of the first dice and $Y$ to the outcome of the 2nd dice.

Here $X$ can have the values 1,2,3,...,6 and $Y$ can have the value 1,2,3,...,12.

In Probability and statistics we say that the variable $X$ can **take** 6 values in the range(1,6) and variable $Y$ can **take** values in the range(1,12). 

The activity of rolling both the dice once is called an **event**, and a collection of such events is called as an **experiment**. These variables $X,Y$ are called the **random variable** as they denote the random outcome of each event in the experiment.

Similarly for the generalized example $R_1, R_2,\cdots R_n$ are the random variables.

### Marginal Probability

Lets say we perform the experiment 10 times and we note the outcome of each experiment.

Lets say we noted that 

| # experiment | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  |
| ------------ | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| $X$          | 1   | 4   | 5   | 3   | 6   | 4   | 1   | 5   | 6   | 1   |
| $Y$          | 1   | 5   | 10  | 2   | 12  | 11  | 10  | 12  | 1   | 10  |

Here we can see that as the occurance of the face values are:

| #face value | 1   | 2   | 3   | 4   | 5   | 6   | 7       | 8       | 9       | 10      | 11      | 12      |
| ----------- | --- | --- | --- | --- | --- | --- | ------- | ------- | ------- | ------- | ------- | ------- |
| $X$         | 3   | 0   | 1   | 2   | 2   | 2   | **n/a** | **n/a** | **n/a** | **n/a** | **n/a** | **n/a** |
| $Y$         | 2   | 1   | 0   | 0   | 1   | 0   | 0       | 0       | 0       | 3       | 1       | 2       |

An interesting property to note is that the sum of all the occurance for both $X$ and $Y$ is 10.

$Sum\ of\ occurances\ of\ a\ random\ variable = \# of\ times\ experiment\ has\ been\ performed$

For the above experiment we can observe that the occurance of $1$ is 3 times for $X$ and 2 times for $Y$ i.e. the probability of $X$ taking the value $1$ is $\frac{3}{10}$ and the probabilty of $Y$ taking the value $1$ is $\frac{2}{10}$.

This can also be written as 

$p(X=1) = \frac{3}{10}$ and 

$p(Y=1) = \frac{2}{10}$

These probability of $X$ and $Y$ are also called the **Marginal Probability**.

So in general we can denote the marginal probability as follows:

$$
p(R_i=a)\ or\ p(a) \tag{1}
$$

### Joint Porbability

The combined porbability of $X$ taking the value 1 and $Y$ taking the value 1 is the number of times both $X$ and $Y$ has taken the value 1. If we look through the table we can find that there is only 1 such occurance, this the probability is $\frac{1}{10}$.

This combined proability is also called the **joint probability** and is written as $p(X=1,Y=1)$.

In general this can be denoted as:

$$
p(R_i=a, R_j=b,\cdots R_k=c)\ or\ p(a,b,\cdots c) \tag{2}
$$

### Conditional Probability

Lets now focus on some complex probabilities.

Lets say we want to find the probability of $Y$ taking the value $10$ when $X$ **has already taken** the value $1$. This can be easily found out by finding the number of occurance of $X$ being 1 and then finding the occurances of $Y$  begin 1 out of these occurances.

We can see that there are 2 such occurances out of 3 thus probability of $Y$ taking value 10 given $X$ is 1 = $\frac{2}{3}$

This is known as the **conditional probability** of $X$ and $Y$. Also noted as $p(Y=10|X=1)$

Similarly in general we can denote the condition probability as:

$$
p(R_i=a,R_k=b,\cdots R_l=c|R_w=f,R_x=g,\cdots R_z=h) \tag{3}
$$

#### Conditional vs Joint Probability

The main difference between conditional $p(Y=10|X=1)$ and joint probability $p(X=1,Y=10)$ is that in joint probability, we check the occurance of both $X=1$ and $Y = 10$ out of all the occurances we have observed in a given set of experiments. Whereas in case of conditional probability we try to find $p(Y=10)$ out of all the occurances of $X=1$ i.e. first we filter out the cases where $X=1$ and then we find the probability of $Y=10$ out of these occurances.  

### Probability Distribution

Probability distributon is just another fancy way of representing the probabilities of each of the outcomes of a random variable. Here in place of numerically stating the probability of each and every single outcome of an experiment, we denote it using either a bar chart or a function.

This representation is often helpful in understanding and performing various numerical operations on the random variable. 

### Basic Rules of Probability

There are 2 basic rules of probability using which we can define the probability for any activity/experiment/occurance.

#### Sum Rule

If we observe from our experiment, the marginal probabilities of $X$ taking the value 1 is actually the sum of all joint probabilities of $X$ and $Y$ where we consider all values of $Y$ keeping $X$ as 1. That is to say that: 

$$
\begin{aligned}
p(X=1) & = \sum_{i=1}^{i=12} p(X=1,Y=i)\\
& = \sum p(X,Y)
\end{aligned}
$$

So we can generalize this as:

$$
\large
p(R_i) = \sum_{n_j,n_k,\cdots n_l}p(R_i,R_j,R_k\cdots R_l) \tag{4}
$$

#### Product Rule

If we observe the conditional probability we can see that $p(Y=10|X=1)$ is basically $\frac{p(X=1,Y=10)}{p(X=1)} = \frac{2/10}{3/10} = \frac{2}{3}$ 

thus we can state that $p(Y=10|X=1)\times p(X=1) = p(X=1,Y=10)$

Genrally speaking:

$$
\tag{5}
p(R_i=a,R_k=b,\cdots|R_w=f,R_x=g,\cdots) = \\
p(R_i=a,R_k=b,\cdots,R_w=f,R_x=g,\cdots)\times p(R_w=f,R_x=g,\cdots) 
$$

### Bayes Theorem

Bayes Theorem derives from the same concept of product rule, just a bit rearranged:

$$
\large
\tag{6}
P(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}
$$

## Note

Here we have considered 10 experiments and a distributon according to that in our examples. The actual distribution of the above example is not the same as the 10 experimental distribution. The actual probabilities of each outcome for the 1st dice is $\frac{1}{6}$ and for the 2nd dice is $\frac{1}{12}$ but we didnt consider this. 

Refer to **Population vs Sample space** notes for the reason.

# Reference

[1]: Book  "Pattern Recogniton and MachineLearning by Christopher M Bishop"
