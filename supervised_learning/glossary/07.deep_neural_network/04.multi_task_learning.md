# Multi-Task Learning

a.k.a

- Joint Learning
- Learning to learn
- Learning with Auxilary tasks

Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning).

We can view multi-task learning as a form of inductive transfer. Inductive transfer can help improve a model by introducing an inductive bias, which causes a model to prefer some hypotheses over others.

In the context of Deep Learning, multi-task learning is typically done with either *hard* or *soft parameter sharing* of hidden layers.

## Hard Parameter Sharing

It is generally applied by sharing the hidden layers between all tasks, while keeping several task-specific output layers. Hard parameter sharing greatly reduces the risk of overfitting.

> The more tasks we are learning simultaneously, the more our model has to find a representation that captures all of the tasks and the less is our chance of overfitting on our original task.



## Soft Parameter Sharing

In soft parameter sharing on the other hand, each task has its own model with its own parameters. The distance between the parameters of the model is then regularized in order to encourage the parameters to be similar.



## Multi Task learning in Non Deep Neural Models

### Block Sparse Regularization

We have ***T~T~*** tasks. For each task t, we have a model ***m~t~*** with parameters ***a~t~ ***of dimensionality ***d***. We can write the parameters as a column vector ***a~t~=[a~1,t~, … a~d,t~]^⊤^***. We now stack these column vectors ***a~1~,…,a~T~*** column by column to form a *matrix $\bold{A∈R^{d×T}}$*.

The ii-th row of AA then contains the parameter a~i~ corresponding to the i^th^ feature of the model for every task, while the j^th^ column of A contains the parameters a~j~ corresponding to the j^th^ model.

Many existing methods make some sparsity assumption with regard to the parameters of our models, assume that all models share a small set of features. 

