# Supervised Learning Pipeline

When we are building a supervised learning model we follow certain pipeline or methods to successfully solve a problem.

There are basically 6 basic stages in the process:

1. Gathering Data

2. Understanding Data or EDA(Exploratory Data Analysis)

3. Model creations based on our EDA

4. For each model:
   
    1. Model Training
   
    2. Validation stage to check how to tweak the model to best fit our performance metric

5. Model Testing and Choosing a model

6. Inferring unknown/unseen data using the model



### 1. Data Gathering

Based on what we are trying to solve using a neural network we need to find the right data set to fit our experiment. We can sometimes find a ready-made data set which perfectly fits our needs or we might need to gather multiple data and process it in a form that we can perform our EDA on it.

In most cases the data set we find, will not directly suit our needs. It can have the following characteristics

- *Incomplete Data*: It might not contain all the data we need, thus we will need to append some extra information depending on the data set.

- *Incomplete Rows*: Most of the time, the data set will contain rows whose all columns might not have any values or might have undesirable values. In this case we will need to impute new values in the specific rows.

- *Unclean Data*: Often the data set will be full of unwanted data, and in certain cases, rows will be incomplete to an extent that, the value which we are trying to predict might not be predictable with the given values for that row. In these cases, the imputation process might not yield any result and we will need to clean the unwanted data.  **

- *Redundant Data:* As stated, rows can be incomplete in most cases, but sometimes the data a column represents itself can be derived from other columns and might be redundant. In these cases, like we the cleaning step, we will need to remove such redundant data so that we are left with only meaningful data and this decreasing our range of information and dimension of the dataset.



### 2. Exploratory Data Analysis (EDA)

After we have obtained a clean and meaningful data, we will need to study it to understand the patterns present in it and the relations between different properties. 

> **This step is the most crucial step as this will dictate which model to choose and which features to exactly focus on.**

EDA process involves visualizing the data using various methods like using t-SNE, confusion matrix, heatmaps and more. These visualization techniques helps us to understand existing patterns.

Once we have grasped and understood our data, we can make a calculated guess about the set of models which can accurately extract the necessary features and learn the patterns.



### 3. Model Creation

This step revolves around the creation of different models, that we guessed can handle the dataset which we are currently working on. Depending on the model, we will know the amount of data we need to train that model.

Before we start training the model, we will need to distribute the data into 3 separate groups,

1. For training (70% usually)

2. For validation (15% to 20% usually)

3. For testing (10% to 15%)

This grouping helps us to concretely test the performance of the model on unseen data.



### 4.A. Model Training

For each model, we perform the model training using the training set and validation step using the validation set together.

Here we use the concept of epochs and batches to train the model. Once the model completes all the epochs, we move on to the validation step to validate the performance of the model on unseen data. 

### 4.B. Model Validation

Once the model is trained we need to verify that the model neither overfits the dataset nor does it underfit the dataset. 

**Overfitting**: In case of overfitting we will observe that the model performs exceptionally well during training but performs poorly during validation.  In such scenarios we use the concept of regularization to achieve better results in the validation step.

Underfitting: In this case the model neither performs well in the training set nor in the validation set. In such scenarios we try to tweak the model's parameters(or hyperparameters) such that the model performs well in the training stage.



### 5. Model Testing and Choosing

Once we are satisfied with each model's training and validation performance, we will start with the testing stage. In this stage we use the testing set and run each model on the testing set. Whichever model performs better in this set is then chosen for the real world applications.



### 6. Data Inference

Once we chosen the model in the testing stage, we start using the model for unknown data also called inference.



---


