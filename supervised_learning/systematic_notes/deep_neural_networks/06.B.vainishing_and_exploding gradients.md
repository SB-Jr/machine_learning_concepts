# Vanishing and Exploding Gradients

When we are using backpropagation to calculate the gradients, we tend to use the chain rule to find the values upto the 1st hidden layer.

The chain rule is basically the multiplication of the various partial derivatives.

> If the terms used are large then multiplying the values at some point can reach way to high value thus known as the exploding gradients.

> On the other hand if the values are way too small, the multiplications will result in a further smaller numbers thus leading to near 0 value also known as vanishing gradients.

The main reason why these are an issue is because when we are porgraming these in python, we will either end up with a NAN value for exploding gradients or 0 for vanishing gradients. This in turn doesn't help the model learn properly and thus might cause issues in fitting the dataset.

These can be the tipping points for letting us know if we have either of these issues:

For exploding gradients:

- Model will have large changes in loss with each iteration

- Model loss will soon become NaN during training

- Model weights are way too high and sometimes become NaN

- Derivatives are constant in each iteration

- Model doesn't learn much even with large dataset resulting in poor performance in training phase



For vanishing gradients:

- Model weights soon achieve 0 value

- The weights near the output will have drastic changes whereas the weights for layer near the input will hardly have any changes in each iteration

- Model will either improve very slowly or will stop early as the loss soon achieves 0 and no changes are detected in loss.



## Basic Solution to these situations are:

- Reducing the Layers of the network

- Gradient clipping i.e. limiting the values the gradient can take. Mainly used for Exploding gradients

- Careful weight initialization can help reduce vanishing gradients but can help with exploding gradients only partially.





# Reference

[1]: <https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11> "Vanishing and Exploding gradients"
