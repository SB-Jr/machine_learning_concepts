# HyperParameter Tuning

When we are done with using various optimization algorithms and regularization we will have many hyperparameters to tune to get the most out of our model.

The hyperparameters can be adjusted using many hyperparameter tuning algorithms.

These algorithms can be divided into 3 sections:

- Exhaustive search of space
- Surrogate models
- Mix and Match of above 2 methods

## Exhaustive Search

These methods tend to try out all the possible ways the hyperparameters can be tuned and train the model so as to get the best fitting hyperparameters.

### Grid Search

Grid search is a very basic method for tuning hyperparameters of neural 
networks. In grid search, models are built for each possible combination
 of the provided values of hyperparameters. These models are then 
evaluated and the one that produces the best results is selected.

Main DIsadvantage:

- It is very inefficient as it exhaustively searches for all the combinations of hyperparameter possible.

### Random Search

In this methods we provide a probability distribution for each hyperparameter and then random sampling is done based on the distribution for each hyperparameter and the corresponding model is evaluated. 

Advantages:

- It performs better than grid search

- Prior knowledge of hyperparameters can help us in tuning the hyperparameters better as we can provide proper distributions for each hyperparameter

- Mainly useful when changing few hyperparameters can affect the model a lot.

Disadvantage:

- It is still an exhaustive search

- A prior knowledge of hyperparameter is required to provide statistical distribution for each.



## Surrogate models

### Bayesian Optimization

It is an implementation of Sequential Model Based Optimization or SBMO. Compared to previous methods, which can be conducted in parallel so as to arrive to specific set of hyperparameter values. In this method, information gained from previous tuning is used to aid the next selection of hyperparameters. 

Advantages:

- The hyperparameter tuning works in a calculated guess rather than selecting things randomly

- It eventually turns to better hyperparameters configuration as the number of times the model is trained and validated is increased

Disadvantage

- The method cant perform the training and validation of multiple iterations in parallel as compared to random and grid search

- An initial configuration is required else it will take a lot of time to attain better performing models.

### Population Based Training(PBT)

This method is inspired from genetic algorithms where the information from existing optimal model's hyperparameter is used to get a starting point and the hyperparameters near to the region of optimal model's hyperparameter is used to further search for the best configuration which suits our model.

Population learning of neural networks, as well as random search, begins
 with parallel learning of the population of neural networks with random
 values ​​of hyperparameters. But, instead of training neural networks 
independently, from time to time, they are interrogated to refine the 
hyperparameters of models, based on the hyperparameters of those models 
that claim to be optimal. Thus this process helps in finding the optimal hyperparameter in the learning process itself rather then re-training and re-evaluating the models from time to time.

Main Disadvantage:

- A previous optimal model is needed to be known so that we can start from some place, else it will take a lot of resources to generate an optimal configuration.

# Reference

[1]: <(https://blog.eduonix.com/software-development/hyperparameter-tuning-neural-networks/)> "Hyperparameter Tuning In Neural Networks"
