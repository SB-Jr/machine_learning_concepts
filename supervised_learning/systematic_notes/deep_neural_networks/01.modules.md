## Neural Network Modules

A neural network architecture contains multiple modules which together makes up the neural network. Each module has multiple implementations and mix-matching these implementations results in multiple forms of neural network.

The following are the neural network modules:

- Node or Perceptron

- Architecture or Node connection layout
  
    - Network Operations
  
    - Forward Propagation

- Loss Function

- Optimization Function

## 1. Node or Perceptron

This is the basic building block of  a neural network model. It acts as a data holder for each layer when we are training a neural network model. The node structure changes for different architectures.

A basic node models comprises of 2 parts:

- Data: Which holds the data coming from the previous layer or the input and

- Activation: The data we get after applying an activation function on the the data received from previous layer.

The perceptron is what makes Neural Network different from a standard machine learning model. The activation function in the perceptron brings a kind of non-linearity into the data , thus able to grasp and capture complex patterns present in the data.

### 2. Architecture

As said earlier, the node changes with respect to the architecture we are using. An artificial neural network uses a basic perceptron whereas a Convolution Neural Network uses convolution nodes.

### 2.A. Operations

The architecture also dictates the operations that will be carried out on the input data(aka features) to get the desired output. An ANN uses the logistic/linear regression operations on each node, to get the data for the next node.

In case of CNN, it uses a combination of 3 operations named:

- Convolution

- Polling

- Padding

### 2.B. Feed Forward

Using the operations defined by the architecture, it then dictates how the data should flow from the input to the output node. This is known as the feed forward, as the data moves forward through the nodes to the output nodes. The applications of operations in the feed forward process changes the input data to resemble the output data. The degree of resemblance is controlled using parameters which are over the training period updated to get a better degree of resemblance.

### 3. Loss Function

The loss function tells us about the degree of difference between our predicted value and the actual value. This helps us in updating the parameters(aka weights). The more the degree of difference, the more aggressively we change the parameters, the lesser the difference the less we change the parameters.

#### 3.A. Cost function

In most cases we will use loss function and cost function interchangeably but it is important to note the difference between them.

> Loss function only tells us the extent to which our model's prediction was wrong. The higher the loss function, the more the model needs to adjust its weights to get a correct prediction. 

> Where as this is fine for 1 data point, but when we are performing the optimization we might be looking are 1 or more examples at the same time. In such situations, we define the cost function.

Cost function can be either equal to the loss function directly, or it can be an average of loss function over many data points or some constant or variable multiplied with the loss function to aid calculations or performance.

### 4. Optimization

Optimization algorithms lays down the steps that is needed to performed to accurately update our model's parameters. It takes into consideration different information before updating the parameters. One of the key information used is the value returned by the cost function(note: not the loss function). Based on the loss function's calculations the optimization algorithm changes the parameters.

----


