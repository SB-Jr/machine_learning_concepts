# Residual Network

## The Problem

### Observation

Usually when we need more accurate results, we tend to increase the depth of the CNN by stacking more convolution layers before we flatten it and use the dense layers. 

The usage of deeper layer does gives us a better result compared to shallow layers, but as the depth keeps increasing, we start to face the problem of **vanishing gradients**.

Usually, when we increase the CNN after a certain point, the error starts to increase rather than decreasing, as a result the deeper layered model start performing poorer compared to the shallower model. This decrease in training error does mean that the model is not learning properly anymore and that it is now underfitting the data rather than overfitting.

### The Reason

The main reason behind this is the use of activation functions. Activation functions are used to squash the value obtained by multiplying the input with weights(and bias added). This squashing of the input into a limited range might help us with the forward propagation.**[Note: This solves the problem of exploding gradients].** But after a certain depth, these activation functions start saturating the gradient growth of the layers nearer to the output layer and prevents the flow of gradient update to the shallow layers i.e. the layers near to the input.

This happens because of the very nature of activation function i.e. squashing the input into a limited output also affects the gradient update value when differentiated. As we know that an activation function has small curve and a small range, thus the gradient obtained on differentiating it in that range is also small. As a result, the error is easily propagated to the layers near to the output but as we go towards the input, the chain rule multiplies all the gradients per layer, thus the error diminishes as it reaches the layers near input.

### The Solution

Usually when we take a model of $L$ layers and we add another layer to it to make it of length $L+1$, we expect that the added layer should either improve the performance at the expense of computation cost or should at the very least not degrade the performance compared to the model with $L$ layers.

Thus what we mathematically expect is that the $(L+1)^{th}$ layer should at the very least act as a identity function so that the output of the $L^{th}$ itself is passed as the output of the $(L+1)^{th}$ layer to produce same loss or should otherwise improve the result by modifying the output such that the loss is reduced.

<img title="" src="assets/residual.png" alt="">

So as to force the learning of an identity function as the least bound scenario, we can use the concept of Residual Neural Network. 

Let's say that the $F(x)$ is the output of a convolution layer with the activation applied on it. Usually this output $F(x)$ is passed onto the next layer. But here we introduce the concept of `shortcut connection`, where the input is again added to the output. So now the new output of the convolution with activation layer with the shortcut connection is $H(x) = F(x) + x$.

If in case the dimensions of input $x$ and output $F(x)$ don't work, then we can apply a $1\times1$ convolution operation on top of input $x$ so that they both are of same dimension. As now we have a modified output $H(x)$, the loss is calculated based on $H(x)$ and not $F(x)$. **Note: F(x) includes the activation function applied before being added to $x$**

So here we are using the shortcut connection as our medium to force the layer to act as the identity function. Because for cases when $F(x)$ is 0, $H(x) = x$

holds true and as a result the layer can't go below a identity function.(**Note: The weights can at max become 0 after multiple backpropagation so the value of $H(x)$ can at least become $x$ and not below that**).

Here the output $F(x)$ is known as the residual term i.e. if we rearrange the formula we get $F(x) = H(x) - x$, i.e. the output $F(x)$ acts as a residual term or subtraction residue.

## Deep Residual Learning

Lets say we have a residual block as the $i^{th}$ layer. Here we as we have seen, the output will be $H(x)$ rather than the simple $F(x)$. But the learning parameters are still present in $F(x)$ so when we try to update the value presented by $H(x)$ we are indirectly updating $F(x)$. 

Lets say $H(x)$ attains the value needed for the least loss, in such scenarios if we had a CNN without residual block then it would have meant that we needed to train the $i^{th}$ block so that the value $H(x) = F(x)$ attains what we need to minimize the error. But as now with the added the residual block, the layer no longer needs to train to attain $H(x)$ but now needs to train enough to attain $H(x)-x$ i.e. the residue, thus the amount of shift needed for the layer is reduced as it only has to learn the residue and not the whole exact value which could be difficult.

Another thing which is elegant about residual network is that the backpropagation still applies to this and that no new complex method is needed to update the weights.

So the loss is properly back propagated to the initial layers as well as the back propagation travels through the shortcut for shallow layers and the term needed for updating the dense layer is only transferred to the dense layer.  

### Backpropagation

Usually in linear models the backpropagation for layer $l$ loos like this:

$$
\begin{aligned}
\triangle w_l &= -\alpha\frac{\partial E}{\partial w_l} \\
&= -\alpha\frac{\partial a_l(w_l\cdot x)}{\partial w_l}\times \text{rest of chain rule}\\
&= -\alpha a'_l(x) \times \text{rest of chain rule}\\
w_l &= w_l + \triangle w_l
\end{aligned}
$$

But when we introduce the residual network, a new term comes into picture when updating the weights of layer $l$ (layer l's input is skipped over layer $l+1$ to create a skip connection) which basically solves the problems with diminishing gradients:

Note: here I have used $I_l(x)$ to denote the input to layer $l$ as a function of input $x$

$$
\begin{aligned}
\triangle w_{l+1} &= -\alpha\frac{\partial E}{\partial w_{l+1}} \\
&= -\alpha \frac{\partial (a_{l+1}(w_{l+1}\cdot I_{l+1}(x)) + a_l(w_l \cdot I_l(x)))}{\partial w_{l+1}} \times \text{rest of chain rule}\\
&= -\alpha(a'_{l+1}(w_{l+1}\cdot I_{l+1}(x))I_{l+1}(x))\times \text{rest of chain rule} \\
w_{l+1} &= w_{l+1} + \triangle w_{l+1}
\end{aligned}
$$

$$
\begin{aligned}
\triangle w_l &= -\alpha \frac{\partial E}{\partial w_l} \\
&= -\alpha\frac{\partial(a_l(w_l\cdot I_l(x)) + a_{l+1}(w_{l+1}\cdot a_l(w_l\cdot I_l(x))))}{\partial w_l} \times \text{rest of chain rule}\\
&= -\alpha(a'_l(w_l\cdot I_l(x))I_l(x) + a'_{l+1}(w_{l+1}a_l(w_l\cdot I_l(x)))\times(w_{l+1}a'_l(w_l\cdot I_l(x))I_l(x))) \times \text{rest of chain rule}\\
w_l &= w_l +\triangle w_l
\end{aligned}
$$

So in this case we get a new term in our gradient term

$$
a'_{l+1}(w_{l+1}a_l(w_l\cdot I_l(x)))\times(w_{l+1}a'_l(w_l\cdot I_l(x))I_l(x))
$$

which is added to help with the diminishing gradient.





# Reference

[1]: <https://arxiv.org/abs/1512.03385> "[1512.03385] Deep Residual Learning for Image Recognition"
