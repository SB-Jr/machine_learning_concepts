# Parameters & Hyperparameters

There are multiple parameters we use to get the desired result from our neural network model. Some of the parameters are used to control the output of the network and some are used to control the various processes taking place while training a model.

**Parameters**: The values which are used during the feed forward process of the model either during the training phase or validation phase or during the testing phase are known as the Parameters. These values are initialized by us before the model training and is the value which the model learns during the training phase.

These values are then used in the validation and testing phase to make a prediction.

We apart from initializing, don't control the parameters in any phase of the model.

**Hyperparameters**: These are the values which we can control and directly affects the way our model will learn the parameters. We use these values to control the degree and extent to which the parameters will be changed during the training process.

The number of parameters and the form of the parameters change with different model architecture.

eg:

- ANN: The parameters are called *weights* and the number of parameters depend on the number of hidden layers and the number of nodes in each layer.

- CNN: The parameters are called *filters* and it like weights depends on the input dimensions(whether input images are RGB or just Black and White) and   the number of layers in model.

The number of hyperparameters and its forms depend on the loss function, optimization algorithm we choose.

eg:

- Learning rate($\alpha$): Dictates the extend of change to be brought on the parameters during training phase.

- Regularization hyperparameter($\lambda$): We introduce the process of regularization to inhibit overfitting of data. The extent of this process is controlled by the hyperparameter $\lambda$.

----


