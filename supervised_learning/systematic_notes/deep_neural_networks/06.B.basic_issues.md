# Neural Network Issues

There are certain issues which we need to address so as to get proper results in a neural network implementation. These factors affect a neural network to an extent that if not addressed might result in no learning by a neural network.

These factors are:

- Vanishing and Exploding Gradients

- Symmetry of weights

## Vanishing and Exploding Gradients

When we are using backpropagation to calculate the gradients, we tend to use the chain rule to find the values upto the 1st hidden layer.

The chain rule is basically the multiplication of the various partial derivatives.

> If the terms used are large then multiplying the values at some point can reach way to high value thus known as the exploding gradients.

> On the other hand if the values are way too small, the multiplications will result in a further smaller numbers thus leading to near 0 value also known as vanishing gradients.

The main reason why these are an issue is because when we are programming these in python, we will either end up with a Nan value for exploding gradients or 0 for vanishing gradients. This in turn doesn't help the model learn properly and thus might cause issues in fitting the dataset.

These can be the tipping points for letting us know if we have either of these issues:

For exploding gradients:

- Model will have large changes in loss with each iteration

- Model loss will soon become Nan during training

- Model weights are way too high and sometimes become Nan

- Derivatives are constant in each iteration

- Model doesn't learn much even with large dataset resulting in poor performance in training phase

For vanishing gradients:

- Model weights soon achieve 0 value

- The weights near the output will have drastic changes whereas the weights for layer near the input will hardly have any changes in each iteration

- Model will either improve very slowly or will stop early as the loss soon achieves 0 and no changes are detected in loss.

### Basic Solution to these situations are:

- Reducing the Layers of the network

- Gradient clipping i.e. limiting the values the gradient can take. Mainly used for Exploding gradients

- Careful weight initialization can help reduce vanishing gradients but can help with exploding gradients only partially.



## Symmetry of Weights

Another issue that haunts a neural network performance is the symmetry of weights. When we don't initialize weights to random values or we initialize all the weights to the same value i.e. some constant $c$, there is no differentiating factors between the hidden units in the same layer. This results in all the units to learn the same function from the previous layer as rest of the computation of each hidden unit is the same i.e $A_{i-1}^T \cdot W + b$. Thus the hidden units fail to recognize different patterns and becomes similar to a hidden layer with 1 unit.

This can be resolved by using a random variable to sample values to initialize the weights or some other methods to assign differentiating weights for each unit.

#### Xavier Initialization

If we look closely at the weights and how it affects the output, we can see that,

for layer $i$ with $n_{in}$ number of units and $j$ representing each unit:

$$
o_i = \sum_j^{n_{in}} x_i * w_{ij}
$$



$$
\begin{aligned}
E[output_i] &= \sum_j^{n_{in}} E[x_j * w_{ij}] &\text{ [where j is the number of nodes in layer i]} \\
&= \sum E[x_j]E[w_{ij}]
\end{aligned}
$$

If we consider that we normalized the input i.e. $E[X] = 0$ we can say that $E[o_i] = 0$ 

Similarly for variance:

$$
\begin{aligned}
Var[output_i] &= E[output_i^2] - E[output_i]^2 \\
&= \sum_j^{n_{in}} E[x_i^2 * w_{ij}^2] - 0 \\
&= \sum E[x_i^2]E[w_{ij}^2] \\
&= n_{in} \gamma^2 \sigma^2
\end{aligned}
$$

Where 

- $n_{in}$ is the no of input,

- $\gamma^2$ is the variance of the input

- $\sigma^2$ is the variance of the output at layer $i$  [given that we have 0 mean for weights as well]

As we can see that if we don't control the variance of the weights, it can blow up and cause gradient explosion.

A simple way to control the gradient is to make sure that $n_{in}\sigma^2 = 1$

Similarly when we are backpropagating we get the term $n_{out}\sigma^2$ and so if again if we don't control the variance, we might end up with gradient explosion. Similarly to control this we can make sure that $n_{out}\sigma^2 = 1$

In all these cases we are not considering the variance of input as that we can control it using normalization. 

This is where the Xavier Initialization comes into picture. It tries to control the variance by maintaining the fact that $\frac{1}{2}(n_{in} + n+{out})\sigma^2 = 1$.

This ensures that gradient explosion doesn't occur by taking the average of both the variances during forward propagation and backward propagation. This gives us the equation:(if we rearrange the above average)

$$
\sigma = \sqrt{\frac{2}{n_{in}+n_{out}}}
$$

By maintaining the variance of the weights in this manner we overcome all the issues.

> Note: Here we didn't consider that the input is using Gaussian distribution. So we can use any distribution as long as we maintain this distribution's  mean as 0 and variance as stated above.



# Reference

[1]: <https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11> "Vanishing and Exploding gradients"

[2]: <(http://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)> "Numerical Stability and Initialization"
