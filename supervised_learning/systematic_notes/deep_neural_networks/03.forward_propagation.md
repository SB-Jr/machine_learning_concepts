# Neural Network Forward Propagation

The data flow from the input nodes to the output nodes is termed as the Feed Forward propagation as the data is flowing in the forward direction (input -> hidden layers -> output)

In feed forward propagation, we perform 2 operation for each node $i$ at layer $l$:

- $z^i_l = \sum_j a_{l-1}^j * w^i_l$, where $a_{l-1}^j$ is the $j^{th}$ node in layer $l-1$

- $a^i_l = g^i_l(z^i_l)$, where $g_l^i(x)$ is the activation function of the $i^{th}$ node at layer $l$

This calculation can also be represented using matrices. For any layer $l$ we can represent the previous layer's data as $A_{l-1})$ , weights for each node for layer $l$ clubbed together as $W_l$ and the layers data as a whole as $A_l$ and $Z_l$.

Thus the equations for layer $l$ is:

- $Z_l = A_{l-1}^T \cdot W_l$ 

- $A_l = g_l(Z_l)$, where $g_l(x)$ is the activation function of the layer $l$

Here the dimensions of the matrices are as follows:

- For input layer i.e. $l=0$ 
  
    - $A_0 \in \mathbb{R}^{m \times n}$ where $m$ is the number of input values or batch size and $n$ is the number of features of the dataset.

- For any hidden layer $l$ with $n_l$ number of nodes and previous layer being $l-1$ and $n_{l-1}$ being the number of nodes in previous layer
  
    - $W_l \in \mathbb{R}^{n_{l-1} \times n_l}$ 
  
    - $Z_l \in \mathbb{R}^{n_l}$
  
    - $A_l \in \mathbb{R}^{n_l}$

- For output layer $L$ 
  
    - $W_L \in \mathbb{R}^{n_{l-1} \times n_L}$
  
    - $Z_L \in \mathbb{R}^{n_L}$
  
    - $A_L \in \mathbb{R}^{n_L}$

## Example

Let say we have 1 hidden layer in a neural network. In this scenario the data at each layer will the following:

- Input layer = $A_0$ 

- Hidden Layer 1:
  
    - $Z_1 = A_0^T \cdot W_1$
  
    - $A_1 = g_1(Z_1)$

- Output Layer:
  
    - $Z_2 = A_1^T \cdot W_2$
  
    - $$
      \begin{aligned}
A_2 = & g_2(Z_2) \\
= & g_2(A_1^T \cdot W_2) \\
= & g_2(g_1(A_0^T \cdot W_1) \cdot W_2)
\end{aligned}
      $$

Here we can see that the output layer which in the end gives us the prediction data is a complicated function with higher degrees of polynomials of the input present.

Our goal of using a neural network is to map the output to a high degree polynomial of  the input and then calculate this mapping using various methods.

As the output we are trying to predict uses a high degree polynomial mapping of the input, we are indirectly calculating the relation between the output and the input using a complicated function. This thus helps us to find real world patterns in the data as the real world patterns are seldom linear.  

To represent this polynomial we are here using  the following to control its complexity:

- weights

- number of hidden layers

- activation function

- network architecture

To find the mapping, our definite goal here is to find the *weights* that we initially choose randomly and then tend to make changes to, to get near to as less difference between the predicted value and real output as possible.

To find this pattern, we are in a way using a calculated hit and trial method. The number of times we shall perform the hit and trial is controlled by the *epoch* parameter. 

> If you look closely, you will see that each neuron in our neural network model actually represents a linear regression/logistic regression model. Thus we can say that a neural network can actually solve what a linear/logistic regression can solve and can do much more. Thus neural networks are a superset of logistic/linear regression.
