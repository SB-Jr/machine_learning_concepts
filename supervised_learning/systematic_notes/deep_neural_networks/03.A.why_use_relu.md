# Why use ReLU?

As we have seen that ReLU is not differentiable for x = 0.

This can come as an alarm while using it as the activation function with backpropagation as the function might return some error while we reach x -> 0 and we try to update our weights by taking the darivatives at that point.

Even though this is true, we usually disregard it while using it in Neural Networks because we never really reach the limit, in general we usually don't arrive at the local minima of the cost function as we don't want our model to overfit and reach the limit. Thus if our cost function's minima corresponds to a undefined gradient, we usually still accept it as we never try to reach it, we just try to reach near it.

On the other hand, the computer implementation of ReLU usually gives us a value of the gradient near 0, rather than evaluating it for 0 and returning an error or NAN. And this is justified as we always roundof some value to 0 and usually dont work around 0 , but a very very small number. Thus when evaluating relu_gradient(0) it is rarely the case that the value was actually 0, so the software implementation usually return a one-sided derivative.

And as we are already familiar with the dying problem in case of relu, we usually use some kind of the bias (b) and regularization such that the node is never deactivated if the value is 0 or -ve.  



# Reference

[1]: https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec
